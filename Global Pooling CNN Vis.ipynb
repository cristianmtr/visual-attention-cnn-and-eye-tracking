{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper:\n",
    "https://arxiv.org/pdf/1512.04150.pdf\n",
    "\n",
    "GitHub:\n",
    "https://github.com/jacobgil/keras-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras import backend as K\n",
    "import h5py\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import argparse\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    return np.expand_dims(X_train, axis=1), np.expand_dims(X_test, axis=1), to_categorical(y_train), to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_poet(img_shape):\n",
    "    print(\"Loading POET dataset...\")\n",
    "    files_plane = glob.glob(r'C:\\Users\\Michal\\Desktop\\cogsci3\\POETdataset\\PascalImages\\aeroplane*')\n",
    "    files_boat = glob.glob(r'C:\\Users\\Michal\\Desktop\\cogsci3\\POETdataset\\PascalImages\\boat*')\n",
    "    files_dog = glob.glob(r'C:\\Users\\Michal\\Desktop\\cogsci3\\POETdataset\\PascalImages\\dog*')\n",
    "\n",
    "    files = files_plane\n",
    "    x1 = np.array([image.img_to_array(image.load_img(file, target_size=img_shape, grayscale=True)) for file in files])\n",
    "    files = files_dog\n",
    "    x2 = np.array([image.img_to_array(image.load_img(file, target_size=img_shape, grayscale=True)) for file in files])\n",
    "\n",
    "    X_images = np.concatenate([x1,x2])\n",
    "    y = np.concatenate([[1]*x1.shape[0],[0]*x2.shape[0]])\n",
    "    X_images.shape, y.shape\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.1, stratify=y)\n",
    "    print(\"Loaded POET dataset.\")\n",
    "#     X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "    return X_train, X_test, to_categorical(y_train), to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_average_pooling(x):\n",
    "    return K.mean(x, axis = (2, 3))\n",
    "\n",
    "def global_average_pooling_shape(input_shape):\n",
    "    return input_shape[0:2]\n",
    "\n",
    "def other_model(input_channels):\n",
    "    ##model building\n",
    "    model = Sequential()\n",
    "    #convolutional layer with rectified linear unit activation\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(1, 64, 64)))\n",
    "    #32 convolution filters used each of size 3x3\n",
    "    #again\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    #64 convolution filters used each of size 3x3\n",
    "    #choose the best features via pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    model.add(Dropout(0.25))\n",
    "    #flatten since too many dimensions, we only want a classification output\n",
    "#     model.add(Flatten())\n",
    "    #fully connected to get all relevant data\n",
    "#     model.add(Dense(128, activation='relu'))\n",
    "    #one more dropout for convergence' sake :) \n",
    "#     model.add(Dropout(0.5))\n",
    "    #output a softmax to squash the matrix into output probabilities\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def VGG16_convolutions(input_channels):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(input_channels,None,None)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "#     model.add(ZeroPadding2D((1, 1)))\n",
    "#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "#     model.add(ZeroPadding2D((1, 1)))\n",
    "#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "#     model.add(ZeroPadding2D((1, 1)))\n",
    "#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "#     model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "#     model.add(ZeroPadding2D((1, 1)))\n",
    "#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "#     model.add(ZeroPadding2D((1, 1)))\n",
    "#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "#     model.add(ZeroPadding2D((1, 1)))\n",
    "#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    return model\n",
    "\n",
    "def get_model(out_classes, input_channels):\n",
    "#     model = VGG16_convolutions(input_channels = input_channels)\n",
    "    model = other_model(input_channels=input_channels)\n",
    "\n",
    "#     model = load_model_weights(model, \"vgg16_weights.h5\")\n",
    "    \n",
    "    model.add(Lambda(global_average_pooling, \n",
    "              output_shape=global_average_pooling_shape))\n",
    "    model.add(Dense(out_classes, activation = 'softmax', init='uniform'))\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def load_model_weights(model, weights_path):\n",
    "    print('Loading model.')\n",
    "    f = h5py.File(weights_path)\n",
    "    if 'nb_layers' in f.attrs.keys():\n",
    "        for k in range(f.attrs['nb_layers']):\n",
    "            if k >= len(model.layers):\n",
    "                # we don't look at the last (fully-connected) layers in the savefile\n",
    "                break\n",
    "            g = f['layer_{}'.format(k)]\n",
    "            weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "            model.layers[k].set_weights(weights)\n",
    "            model.layers[k].trainable = False\n",
    "    f.close()\n",
    "    print('Model loaded.')\n",
    "    return model\n",
    "\n",
    "def get_output_layer(model, layer_name):\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "    layer = layer_dict[layer_name]\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(poet=True, img_shape=(28,28)):\n",
    "    if poet:\n",
    "        model = get_model(out_classes = 2, input_channels = 1)\n",
    "        X_train, _, y_train, _ = load_poet(img_shape=img_shape)\n",
    "    else:\n",
    "        model = get_model(out_classes = 10, input_channels = 1)\n",
    "        X_train, _, y_train, _ = load_mnist()\n",
    "        \n",
    "    print(\"Training..\")\n",
    "    checkpoint_path=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "    model.fit(X_train, y_train, nb_epoch=40, batch_size=64, validation_split=0.2, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "def visualize_class_activation_map(model_path, img, output_path):\n",
    "        model = load_model(model_path)\n",
    "#         original_img = cv2.imread(img_path, 1)\n",
    "        original_img = img.reshape(img.shape)\n",
    "        width, height, _ = original_img.shape\n",
    "        original_img = np.expand_dims(original_img, axis=0)\n",
    "\n",
    "        #Reshape to the network input shape (3, w, h).\n",
    "        img = np.array([np.transpose(np.float32(original_img), (2, 0, 1))])\n",
    "        \n",
    "        #Get the 512 input weights to the softmax.\n",
    "        class_weights = model.layers[-1].get_weights()[0]\n",
    "        final_conv_layer = get_output_layer(model, \"conv3_3\")\n",
    "        get_output = K.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n",
    "        [conv_outputs, predictions] = get_output([original_img])\n",
    "        conv_outputs = conv_outputs[0, :, :, :]\n",
    "\n",
    "        #Create the class activation map.\n",
    "        cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\n",
    "        for i, w in enumerate(class_weights[:, 1]):\n",
    "                cam += w * conv_outputs[i, :, :]\n",
    "        print(\"predictions\", predictions)\n",
    "        cam /= np.max(cam)\n",
    "        cam = cv2.resize(cam, (height, width))\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "        heatmap[np.where(cam < 0.2)] = 0\n",
    "        img = heatmap*0.5 + original_img\n",
    "        cv2.imwrite(output_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:78: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, activation=\"softmax\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading POET dataset...\n",
      "Loaded POET dataset.\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1384 samples, validate on 346 samples\n",
      "Epoch 1/40\n",
      "1384/1384 [==============================] - 66s 48ms/step - loss: 10.1336 - acc: 0.3548 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 2/40\n",
      "1384/1384 [==============================] - 63s 45ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 3/40\n",
      "1384/1384 [==============================] - 63s 45ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 4/40\n",
      "1384/1384 [==============================] - 68s 49ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 5/40\n",
      "1384/1384 [==============================] - 72s 52ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 6/40\n",
      "1384/1384 [==============================] - 73s 53ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 7/40\n",
      "1384/1384 [==============================] - 73s 52ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 8/40\n",
      "1384/1384 [==============================] - 73s 53ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 9/40\n",
      "1384/1384 [==============================] - 71s 51ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 10/40\n",
      "1384/1384 [==============================] - 70s 50ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 11/40\n",
      "1384/1384 [==============================] - 67s 49ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 12/40\n",
      "1384/1384 [==============================] - 67s 49ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 13/40\n",
      "1384/1384 [==============================] - 66s 48ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 14/40\n",
      "1384/1384 [==============================] - 67s 48ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 15/40\n",
      "1384/1384 [==============================] - 67s 48ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 16/40\n",
      "1384/1384 [==============================] - 67s 49ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 17/40\n",
      "1384/1384 [==============================] - 66s 47ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 18/40\n",
      "1384/1384 [==============================] - 70s 51ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 19/40\n",
      "1384/1384 [==============================] - 73s 53ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 20/40\n",
      "1384/1384 [==============================] - 67s 49ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 21/40\n",
      "1384/1384 [==============================] - 66s 47ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 22/40\n",
      "1384/1384 [==============================] - 65s 47ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 23/40\n",
      "1384/1384 [==============================] - 65s 47ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 24/40\n",
      "1384/1384 [==============================] - 65s 47ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 25/40\n",
      "1384/1384 [==============================] - 66s 48ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 26/40\n",
      "1384/1384 [==============================] - 65s 47ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 27/40\n",
      "1384/1384 [==============================] - 65s 47ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 28/40\n",
      "1384/1384 [==============================] - 65s 47ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 29/40\n",
      "1384/1384 [==============================] - 67s 48ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 30/40\n",
      "1384/1384 [==============================] - 116s 84ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 31/40\n",
      "1384/1384 [==============================] - 110s 80ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 32/40\n",
      "1384/1384 [==============================] - 110s 79ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 33/40\n",
      "1384/1384 [==============================] - 110s 79ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 34/40\n",
      "1384/1384 [==============================] - 109s 79ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 35/40\n",
      "1384/1384 [==============================] - 110s 79ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 36/40\n",
      "1384/1384 [==============================] - 110s 80ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 37/40\n",
      "1384/1384 [==============================] - 110s 80ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 38/40\n",
      "1384/1384 [==============================] - 109s 79ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 39/40\n",
      "1384/1384 [==============================] - 110s 80ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n",
      "Epoch 40/40\n",
      "1384/1384 [==============================] - 110s 80ms/step - loss: 10.4931 - acc: 0.3490 - val_loss: 10.7143 - val_acc: 0.3353\n"
     ]
    }
   ],
   "source": [
    "img_shape = (64,64)\n",
    "train(poet=True, img_shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 128 is out of bounds for axis 0 with size 128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-458e09794534>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvisualize_class_activation_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"weights.02-6.43.hdf5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'poet0.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-78a5b53e2f21>\u001b[0m in \u001b[0;36mvisualize_class_activation_map\u001b[1;34m(model_path, img, output_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mcam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mcam\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mconv_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predictions\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mcam\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 128 is out of bounds for axis 0 with size 128"
     ]
    }
   ],
   "source": [
    "visualize_class_activation_map(model_path=\"weights.02-6.43.hdf5\", img=X[0], output_path='poet0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The shape of the input to \"Flatten\" is not fully defined (got (64, None, None). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-a5170df45eb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimg_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-5f5fce2be423>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(poet, img_shape)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpoet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_poet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-7a9dc30fb79f>\u001b[0m in \u001b[0;36mget_model\u001b[1;34m(out_classes, input_channels)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m#     model = VGG16_convolutions(input_channels = input_channels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m#     model = load_model_weights(model, \"vgg16_weights.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-7a9dc30fb79f>\u001b[0m in \u001b[0;36mother_model\u001b[1;34m(input_channels)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#flatten since too many dimensions, we only want a classification output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m#fully connected to get all relevant data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    490\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[0;32m    491\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;31m# Inferring the output shape is only relevant for Theano.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m                 \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    488\u001b[0m             raise ValueError('The shape of the input to \"Flatten\" '\n\u001b[0;32m    489\u001b[0m                              \u001b[1;34m'is not fully defined '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                              \u001b[1;34m'(got '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                              \u001b[1;34m'Make sure to pass a complete \"input_shape\" '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m                              \u001b[1;34m'or \"batch_input_shape\" argument to the first '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The shape of the input to \"Flatten\" is not fully defined (got (64, None, None). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model."
     ]
    }
   ],
   "source": [
    "img_shape = (64,64)\n",
    "train(poet=True, img_shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading POET dataset...\n",
      "Loaded POET dataset.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 64, 3, 64) for Tensor 'zero_padding2d_48_input_3:0', which has shape '(?, 3, ?, ?)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-09290c8580c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_poet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvisualize_class_activation_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"weights.02-6.43.hdf5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'poet0.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-7b6b34e89424>\u001b[0m in \u001b[0;36mvisualize_class_activation_map\u001b[1;34m(model_path, img, output_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mfinal_conv_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_output_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"conv2_2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mget_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfinal_conv_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mconv_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mconv_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1114\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1116\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1117\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 64, 3, 64) for Tensor 'zero_padding2d_48_input_3:0', which has shape '(?, 3, ?, ?)'"
     ]
    }
   ],
   "source": [
    "X, _, y, _ = load_poet(img_shape=img_shape)\n",
    "visualize_class_activation_map(model_path=\"weights.02-6.43.hdf5\", img=X[0], output_path='poet0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
