{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper:\n",
    "https://arxiv.org/pdf/1512.04150.pdf\n",
    "\n",
    "GitHub:\n",
    "https://github.com/jacobgil/keras-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras import backend as K\n",
    "import h5py\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import argparse\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "def load_mnist():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    return np.expand_dims(X_train, axis=3), np.expand_dims(X_test, axis=3), to_categorical(y_train), to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (244,244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_poet(img_size, input_channels=3):\n",
    "    def load_images(files, grayscale=False):\n",
    "        if grayscale:\n",
    "            return preprocess_input(np.array([image.img_to_array(image.load_img(file, target_size=img_size, color_mode='grayscale')) \n",
    "                             for file in files]))\n",
    "        else:\n",
    "            return preprocess_input(np.array([image.img_to_array(image.load_img(file, target_size=img_size)) \n",
    "                             for file in files]))\n",
    "    \n",
    "    main_path = r'./POETdataset/PascalImages/'\n",
    "    grayscale = True\n",
    "    if input_channels == 3:\n",
    "        grayscale = False\n",
    "    \n",
    "    print(\"Loading POET dataset...\")\n",
    "    \n",
    "    classes = ['aeroplane', 'boat', 'dog', 'bicycle', 'cat', 'cow', 'diningtable', 'horse', 'motorbike','sofa']\n",
    "    classes = [word + '*' for word in classes]\n",
    "    files_list = [glob.glob(main_path + class_) for class_ in classes]\n",
    "    \n",
    "    for files in files_list:\n",
    "        assert len(files) > 0\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    class_map = {}\n",
    "    files_dict = {class_name: class_files for class_name, class_files in zip(classes, files_list)}\n",
    "    for class_number, (class_name, files) in enumerate(files_dict.items()):\n",
    "        x.append(load_images(files))\n",
    "        y.append([class_number] * len(files))\n",
    "        print(class_name, class_number)\n",
    "        class_map[class_name] = class_number\n",
    "\n",
    "    X_images = np.concatenate(x)\n",
    "    y = np.concatenate(y)\n",
    "    print('X.shape:', X_images.shape, 'y.shape:', y.shape)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.01, stratify=y)\n",
    "    print(\"Loaded POET dataset.\")\n",
    "#     X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "#     return X_train, X_test, to_categorical(y_train), to_categorical(y_test), class_map\n",
    "    return X_images, to_categorical(y), class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_average_pooling(x):\n",
    "#     print(x.shape)\n",
    "    return K.mean(x, axis = (1, 2))\n",
    "\n",
    "def global_average_pooling_shape(input_shape):\n",
    "#     print(input_shape)\n",
    "    return input_shape[2:4]\n",
    "\n",
    "def other_model(img_shape):\n",
    "    ##model building\n",
    "    model = Sequential()\n",
    "    #convolutional layer with rectified linear unit activation\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=img_shape))\n",
    "    #32 convolution filters used each of size 3x3\n",
    "    #again\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    #64 convolution filters used each of size 3x3\n",
    "    #choose the best features via pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    #flatten since too many dimensions, we only want a classification output\n",
    "#     model.add(Flatten())\n",
    "    #fully connected to get all relevant data\n",
    "#     model.add(Dense(128, activation='relu'))\n",
    "    #one more dropout for convergence' sake :) \n",
    "#     model.add(Dropout(0.5))\n",
    "    #output a softmax to squash the matrix into output probabilities\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def VGG16_convolutions(img_shape):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=img_shape))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    return model\n",
    "\n",
    "def resnet():\n",
    "    return ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "def get_model(out_classes, img_shape):\n",
    "    model = VGG16_convolutions(img_shape=img_shape)\n",
    "    model = load_model_weights(model, \"./vgg.hdf5\")\n",
    "    \n",
    "#     model = resnet()\n",
    "    for layer in model.layers[:]:\n",
    "        layer.trainable = False\n",
    "\n",
    "#     model = other_model(img_shape=img_shape)\n",
    "        \n",
    "    model.layers[-1].name = 'last_conv'\n",
    "    \n",
    "    model.add(Lambda(global_average_pooling, \n",
    "              output_shape=global_average_pooling_shape))\n",
    "    model.add(Dense(out_classes, activation = 'softmax'))\n",
    "#     model = load_model_weights(model, \"./weights/vggweights.04-8.53.hdf5\")\n",
    "\n",
    "    optm = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "#     optm = Adam()\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = optm, metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    for layer in model.layers:\n",
    "        print(layer.name, layer.trainable)\n",
    "    return model\n",
    "\n",
    "def load_model_weights(model, weights_path):\n",
    "    print('Loading model.')\n",
    "#     f = h5py.File(weights_path)\n",
    "#     print(f, f.attrs.keys())\n",
    "#     if 'nb_layers' in f.attrs.keys():\n",
    "#         for k in range(f.attrs['nb_layers']):\n",
    "#             if k >= len(model.layers):\n",
    "#                 # we don't look at the last (fully-connected) layers in the savefile\n",
    "#                 break\n",
    "#             g = f['layer_{}'.format(k)]\n",
    "#             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "#             model.layers[k].set_weights(weights)\n",
    "#             model.layers[k].trainable = False\n",
    "#         f.close()\n",
    "#         print('Model loaded.')\n",
    "#     else:\n",
    "#         print(\"MODEL NOT LOADED\")\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_output_layer(model, layer_name):\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "    layer = layer_dict[layer_name]\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classes_n=3, poet=True, img_size=(28,28), input_channels=1, epochs_n=40, batch_size=64):\n",
    "    img_shape = (img_size[0], img_size[1], input_channels)\n",
    "    if poet:\n",
    "        model = get_model(out_classes = classes_n, img_shape=img_shape)\n",
    "        X_train, y_train, class_map = load_poet(img_size=img_size, input_channels=input_channels)\n",
    "    else:\n",
    "        model = get_model(out_classes = 10, input_channels = 1, img_size=img_size)\n",
    "        X_train, _, y_train, _ = load_mnist()\n",
    "        \n",
    "    print(\"Training..\")\n",
    "    checkpoint_path=\"./weights/vggweights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "    history = model.fit(X_train, y_train, epochs=epochs_n, batch_size=batch_size, validation_split=0.1, verbose=1, callbacks=[checkpoint])\n",
    "    \n",
    "    return history, X_train, y_train, class_map\n",
    "    \n",
    "def visualize_class_activation_map(model_path, img, output_path):\n",
    "    model = load_model(model_path)\n",
    "#         original_img = cv2.imread(img_path, 1)\n",
    "    original_img = img.reshape(img.shape)\n",
    "    width, height, _ = original_img.shape\n",
    "    original_img = np.expand_dims(original_img, axis=0)\n",
    "\n",
    "    #Reshape to the network input shape (3, w, h).\n",
    "    img = np.array([np.transpose(np.float32(original_img), (2, 0, 1))])\n",
    "\n",
    "    #Get the 512 input weights to the softmax.\n",
    "    class_weights = model.layers[-1].get_weights()[0]\n",
    "    final_conv_layer = get_output_layer(model, \"last_conv\")\n",
    "    get_output = K.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n",
    "    [conv_outputs, predictions] = get_output([original_img])\n",
    "    conv_outputs = conv_outputs[0, :, :, :]\n",
    "\n",
    "    #Create the class activation map.\n",
    "    cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\n",
    "    for i, w in enumerate(class_weights[:, 1]):\n",
    "            cam += w * conv_outputs[i, :, :]\n",
    "    print(\"predictions\", predictions)\n",
    "    cam /= np.max(cam)\n",
    "    cam = cv2.resize(cam, (height, width))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "    heatmap[np.where(cam < 0.2)] = 0\n",
    "    img = heatmap*0.5 + original_img\n",
    "    cv2.imwrite(output_path, img)\n",
    "    \n",
    "def visualize_class_activation_map_org(model_path, original_img, output_path, inv_class_map):\n",
    "    model = load_model(model_path)\n",
    "    width, height, _ = original_img.shape\n",
    "\n",
    "    img = np.array([np.transpose(np.float32(original_img), (0, 1, 2))])\n",
    "\n",
    "    class_weights = model.layers[-1].get_weights()[0]\n",
    "    final_conv_layer = get_output_layer(model, \"last_conv\")\n",
    "    get_output = K.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n",
    "    [conv_outputs, predictions] = get_output([img])\n",
    "    print(conv_outputs.shape)\n",
    "    conv_outputs = conv_outputs[0, :, :, :]\n",
    "    print(conv_outputs.shape)\n",
    "    print(conv_outputs.shape[0:2])\n",
    "    print('class weights:', class_weights.shape)\n",
    "\n",
    "    class_n = np.argmax(predictions)\n",
    "    print(inv_class_map[class_n])\n",
    "\n",
    "    #Create the class activation map.\n",
    "    cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])\n",
    "    for i, w in enumerate(class_weights[:, class_n]):\n",
    "        cam += w * conv_outputs[:, :, i]\n",
    "    print(\"predictions\", predictions)\n",
    "    cam /= np.max(cam)\n",
    "    cam = cv2.resize(cam, (height, width))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "    heatmap[np.where(cam < 0.4)] = 0\n",
    "    img = (heatmap*0.5 + original_img)\n",
    "    img /= np.max(img)\n",
    "    print(np.max(img), np.min(img))\n",
    "    plt.imshow(img)\n",
    "#     cv2.imwrite(output_path, img)\n",
    "#     cv2.imshow('test', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv1_1\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv1_2\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv2_1\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv2_2\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_1\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_2\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_3\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_1\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_2\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:70: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_3\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_1\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_2\")`\n",
      "/home/mikey/.local/lib/python3.6/site-packages/ipykernel_launcher.py:78: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_3\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_1 (ZeroPaddin (None, 246, 246, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 244, 244, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 246, 246, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 244, 244, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 122, 122, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 124, 124, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 122, 122, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 124, 124, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 122, 122, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 61, 61, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 63, 63, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 61, 61, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 63, 63, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 61, 61, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 63, 63, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 61, 61, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 30, 30, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 30, 30, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 30, 30, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "last_conv (Conv2D)           (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (15, 512)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (15, 10)                  5130      \n",
      "=================================================================\n",
      "Total params: 14,719,818\n",
      "Trainable params: 4,724,746\n",
      "Non-trainable params: 9,995,072\n",
      "_________________________________________________________________\n",
      "None\n",
      "zero_padding2d_1 False\n",
      "conv1_1 False\n",
      "zero_padding2d_2 False\n",
      "conv1_2 False\n",
      "max_pooling2d_1 False\n",
      "zero_padding2d_3 False\n",
      "conv2_1 False\n",
      "zero_padding2d_4 False\n",
      "conv2_2 False\n",
      "max_pooling2d_2 False\n",
      "zero_padding2d_5 False\n",
      "conv3_1 False\n",
      "zero_padding2d_6 False\n",
      "conv3_2 False\n",
      "zero_padding2d_7 False\n",
      "conv3_3 False\n",
      "max_pooling2d_3 False\n",
      "zero_padding2d_8 False\n",
      "conv4_1 False\n",
      "zero_padding2d_9 False\n",
      "conv4_2 False\n",
      "zero_padding2d_10 False\n",
      "conv4_3 False\n",
      "max_pooling2d_4 False\n",
      "zero_padding2d_11 False\n",
      "conv5_1 False\n",
      "zero_padding2d_12 False\n",
      "conv5_2 True\n",
      "zero_padding2d_13 True\n",
      "last_conv True\n",
      "lambda_1 True\n",
      "dense_1 True\n",
      "Loading POET dataset...\n",
      "aeroplane* 0\n",
      "boat* 1\n",
      "dog* 2\n",
      "bicycle* 3\n",
      "cat* 4\n",
      "cow* 5\n",
      "diningtable* 6\n",
      "horse* 7\n",
      "motorbike* 8\n",
      "sofa* 9\n",
      "X.shape: (6270, 244, 244, 3) y.shape: (6270,)\n",
      "Loaded POET dataset.\n",
      "Training..\n",
      "Train on 5643 samples, validate on 627 samples\n",
      "Epoch 1/200\n",
      "5643/5643 [==============================] - 39s 7ms/step - loss: 0.4277 - acc: 0.8683 - val_loss: 7.9502 - val_acc: 0.1962\n",
      "Epoch 2/200\n",
      "5643/5643 [==============================] - 36s 6ms/step - loss: 0.2568 - acc: 0.9174 - val_loss: 9.9241 - val_acc: 0.2217\n",
      "Epoch 3/200\n",
      "5643/5643 [==============================] - 36s 6ms/step - loss: 0.1807 - acc: 0.9454 - val_loss: 10.4377 - val_acc: 0.1978\n",
      "Epoch 4/200\n",
      " 768/5643 [===>..........................] - ETA: 28s - loss: 0.1529 - acc: 0.9583"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-780f9fa23227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-7a8904b3a11a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(classes_n, poet, img_size, input_channels, epochs_n, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./weights/vggweights.{epoch:02d}-{val_loss:.2f}.hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist, X, y, class_map = train(classes_n = 10, poet=True, img_size=img_size, input_channels=3, epochs_n=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_class_activation_map_org(model_path=\"./weights/vggweights.15-10.04.hdf5\", original_img=X[5551], output_path='poet0.jpg', inv_class_map=inv_class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading POET dataset...\n",
      "aeroplane* 0\n",
      "boat* 1\n",
      "dog* 2\n",
      "bicycle* 3\n",
      "cat* 4\n",
      "cow* 5\n",
      "diningtable* 6\n",
      "horse* 7\n",
      "motorbike* 8\n",
      "sofa* 9\n",
      "X.shape: (6270, 128, 128, 3) y.shape: (6270,)\n",
      "Loaded POET dataset.\n"
     ]
    }
   ],
   "source": [
    "X, y, class_map = load_poet(img_size=img_size)\n",
    "inv_class_map = inv_map = {v: k for k, v in class_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "visualize_class_activation_map_org() missing 1 required positional argument: 'inv_class_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-a9f331e26316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_class_activation_map_org\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./weights/weights.38-0.81.hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'poet0.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: visualize_class_activation_map_org() missing 1 required positional argument: 'inv_class_map'"
     ]
    }
   ],
   "source": [
    "visualize_class_activation_map_org(model_path=\"./weights/weights.38-0.81.hdf5\", original_img=X[2], output_path='poet0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading POET dataset...\n",
      "Loaded POET dataset.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 64, 3, 64) for Tensor 'zero_padding2d_48_input_3:0', which has shape '(?, 3, ?, ?)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-09290c8580c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_poet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvisualize_class_activation_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"weights.02-6.43.hdf5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'poet0.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-7b6b34e89424>\u001b[0m in \u001b[0;36mvisualize_class_activation_map\u001b[1;34m(model_path, img, output_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mfinal_conv_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_output_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"conv2_2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mget_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfinal_conv_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mconv_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mconv_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1114\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1116\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1117\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 64, 3, 64) for Tensor 'zero_padding2d_48_input_3:0', which has shape '(?, 3, ?, ?)'"
     ]
    }
   ],
   "source": [
    "visualize_class_activation_map(model_path=\"weights.02-6.43.hdf5\", img=X[0], output_path='poet0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "model = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vgg = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vgg.save_weights('./vgg.h5')\n",
    "model.load_weights('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
