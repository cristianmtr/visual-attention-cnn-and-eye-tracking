{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import glob, os, sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50 as resnet\n",
    "from keras.applications.resnet50 import preprocess_input as resnet_pp\n",
    "\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = r'./POETdataset/PascalImages/'\n",
    "classes = ['aeroplane', 'boat', 'dog', 'bicycle', 'cat', 'cow', 'diningtable', 'horse', 'motorbike','sofa']\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model by adding preprocessing before the pretrained CNN\n",
    "def get_feature_extraction_model():\n",
    "    cnn_object, pp_function, img_size = _get_pretrained_model()\n",
    "    model = keras.models.Sequential()\n",
    "    cnn_model = cnn_object(weights='imagenet', include_top=False, pooling='max')\n",
    "    model.add(keras.layers.Lambda(pp_function, name='preprocessing', input_shape=(img_size, img_size, 3)))\n",
    "    model.add(cnn_model)\n",
    "    return model\n",
    "\n",
    "# Unpacking information from the models dictionary\n",
    "def _get_pretrained_model():\n",
    "    cnn_object = resnet\n",
    "    pp_function = resnet_pp\n",
    "    img_size = IMG_SIZE\n",
    "    return cnn_object, pp_function, img_size\n",
    "\n",
    "def get_features(files, model):\n",
    "    # Load images based on the size of the Lambda layer \n",
    "    # provided as the first layer before the pretrained CNN\n",
    "    x = np.array([image.img_to_array(image.load_img(file, target_size=(model.layers[0].input_shape[1], model.layers[0].input_shape[1]))) for file in files])\n",
    "    return model.predict(x, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_feature_extraction_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeroplane*\n",
      "666/666 [==============================] - 4s 6ms/step\n",
      "(666, 2048)\n",
      "boat*\n",
      "504/504 [==============================] - 2s 4ms/step\n",
      "(504, 2048)\n",
      "dog*\n",
      "1257/1257 [==============================] - 5s 4ms/step\n",
      "(1257, 2048)\n",
      "bicycle*\n",
      "536/536 [==============================] - 2s 4ms/step\n",
      "(536, 2048)\n",
      "cat*\n",
      "1051/1051 [==============================] - 4s 4ms/step\n",
      "(1051, 2048)\n",
      "cow*\n",
      "301/301 [==============================] - 1s 4ms/step\n",
      "(301, 2048)\n",
      "diningtable*\n",
      "498/498 [==============================] - 2s 4ms/step\n",
      "(498, 2048)\n",
      "horse*\n",
      "480/480 [==============================] - 2s 4ms/step\n",
      "(480, 2048)\n",
      "motorbike*\n",
      "510/510 [==============================] - 2s 4ms/step\n",
      "(510, 2048)\n",
      "sofa*\n",
      "467/467 [==============================] - 2s 4ms/step\n",
      "(467, 2048)\n"
     ]
    }
   ],
   "source": [
    "classes = [word + '*' for word in classes]\n",
    "files_list = [glob.glob(main_path + class_) for class_ in classes]\n",
    "\n",
    "for files in files_list:\n",
    "    assert len(files) > 0\n",
    "\n",
    "files_dict = {class_name: class_files for class_name, class_files in zip(classes, files_list)}\n",
    "for class_number, (class_name, files) in enumerate(files_dict.items()):\n",
    "    print(class_name)\n",
    "    features = get_features(files, model)\n",
    "    print(features.shape)\n",
    "    save_object(features, class_name+'_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(666, 2048)\n",
      "(504, 2048)\n",
      "(1257, 2048)\n",
      "(536, 2048)\n",
      "(1051, 2048)\n",
      "(301, 2048)\n",
      "(498, 2048)\n",
      "(480, 2048)\n",
      "(510, 2048)\n",
      "(467, 2048)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((6270, 2048), (6270, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for class_number, (class_name, files) in enumerate(files_dict.items()):\n",
    "    features = load_object(class_name + '_features.pkl').reshape(-1,2048)\n",
    "    print(features.shape)\n",
    "    X.append(features)\n",
    "    y.append([class_number] * features.shape[0])\n",
    "    \n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)\n",
    "y = keras.utils.to_categorical(y)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5643, 2048), (5643, 10), (627, 2048), (627, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5643 samples, validate on 627 samples\n",
      "Epoch 1/100\n",
      "5643/5643 [==============================] - 1s 221us/step - loss: 2.5196 - acc: 0.6015 - val_loss: 0.7523 - val_acc: 0.7879\n",
      "Epoch 2/100\n",
      "5643/5643 [==============================] - 0s 80us/step - loss: 0.8148 - acc: 0.7399 - val_loss: 0.6526 - val_acc: 0.8054\n",
      "Epoch 3/100\n",
      "5643/5643 [==============================] - 0s 84us/step - loss: 0.7043 - acc: 0.7647 - val_loss: 0.6460 - val_acc: 0.8150\n",
      "Epoch 4/100\n",
      "5643/5643 [==============================] - 0s 86us/step - loss: 0.6093 - acc: 0.7962 - val_loss: 0.6154 - val_acc: 0.8134\n",
      "Epoch 5/100\n",
      "5643/5643 [==============================] - 0s 79us/step - loss: 0.5412 - acc: 0.8207 - val_loss: 0.5007 - val_acc: 0.8517\n",
      "Epoch 6/100\n",
      "5643/5643 [==============================] - 0s 82us/step - loss: 0.5174 - acc: 0.8293 - val_loss: 0.6412 - val_acc: 0.8278\n",
      "Epoch 7/100\n",
      "5643/5643 [==============================] - 0s 80us/step - loss: 0.4856 - acc: 0.8288 - val_loss: 0.5267 - val_acc: 0.8469\n",
      "Epoch 8/100\n",
      "5643/5643 [==============================] - 0s 86us/step - loss: 0.4620 - acc: 0.8410 - val_loss: 0.5807 - val_acc: 0.8517\n",
      "Epoch 9/100\n",
      "5472/5643 [============================>.] - ETA: 0s - loss: 0.4158 - acc: 0.8560"
     ]
    }
   ],
   "source": [
    "dense_model = Sequential()\n",
    "# model.add(Flatten(input_shape=(1,1,2048)))\n",
    "dense_model.add(Dense(units=128, activation='relu', input_shape=(2048,)))\n",
    "dense_model.add(Dropout(0.25))\n",
    "dense_model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "dense_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(),\n",
    "             metrics=['accuracy'])\n",
    "history = dense_model.fit(X_train,y_train,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 100,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
