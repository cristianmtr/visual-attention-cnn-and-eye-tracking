{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import glob\n",
    "import sys\n",
    "from scipy import io\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import keras\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "POET_DIR = \"D:/data/POETdataset/\"\n",
    "PATCHES_FILE = os.path.join(POET_DIR, \"x.npy\")\n",
    "LABELS_FILE = os.path.join(POET_DIR, 'y.npy')\n",
    "pascal_images = os.path.join(POET_DIR, 'PascalImages')\n",
    "PATCH_SIZE = 32\n",
    "HALF_PATCH = PATCH_SIZE//2\n",
    "DATASET_SIZE = 6270\n",
    "VALIDATION_PERC = 0.1\n",
    "MAX_GAZE_POINTS = 3 # TEMPORARY\n",
    "MIN_GAZE_POINTS = 3\n",
    "VGG_NR_FEATURES = 512\n",
    "user_index = 0 # TEMPORARY\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(POET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATCH EXTRACTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [a.split(\"_\")[1].split(\".mat\")[0] for a in glob.glob(\"etData/*\")]\n",
    "idx2class = {i:c for i, c in enumerate(classes)}\n",
    "class2idx = {c:i for i, c in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixation_within_image(fx, fy, dims):\n",
    "#   print(dims)\n",
    "  if fx < 0:\n",
    "#     print(fx, fy, 'not within')\n",
    "    return False\n",
    "  if fx > dims[0]:\n",
    "#     print(fx, fy, 'not within')\n",
    "    return False\n",
    "  if fy < 0:\n",
    "#     print(fx, fy, 'not within')\n",
    "    return False\n",
    "  if fy > dims[1]:\n",
    "#     print(fx, fy, 'not within')\n",
    "    return False\n",
    "\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_fixations(fixR, fixL, dims):\n",
    "  # ger average of fixations between right and left eye\n",
    "  fix = []\n",
    "  for i in range(max(len(fixR),3)):\n",
    "    fR = fixR[i]\n",
    "    fL = fixL[i]\n",
    "    # no fixations outside\n",
    "    fx = np.mean([fR[0],fL[0]])\n",
    "    fy = np.mean([fR[1],fL[1]])\n",
    "    if fixation_within_image(fx, fy, dims):\n",
    "      fix.append([fx,fy])\n",
    "\n",
    "  fix = np.array(fix)\n",
    "  return fix[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixations(filename, classname, dims):\n",
    "  filename = 'sofa_2010_004095'\n",
    "  classname = 'sofa'\n",
    "  if classname in filename:\n",
    "    filename = filename.split(\"%s_\" %classname)[1]\n",
    "  c_instances = io.loadmat(os.path.join(POET_DIR,'etData','etData_%s.mat' %classname), squeeze_me=True)['etData']\n",
    "  user_index = 1\n",
    "  for i in c_instances:\n",
    "    if filename == i['filename']:\n",
    "      fixR = i['fixations'][user_index]['imgCoord']['fixR'].tolist()['pos'].tolist()\n",
    "      fixL = i['fixations'][user_index]['imgCoord']['fixL'].tolist()['pos'].tolist()\n",
    "      fix = get_avg_fixations(fixR, fixL, dims)\n",
    "      return fix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_patches(filename, classname):\n",
    "  if classname in filename:\n",
    "    filename = filename.split(\"%s_\" %classname)[1]\n",
    "\n",
    "  img_src = cv2.imread(os.path.join(POET_DIR, 'PascalImages', '%s_%s.jpg' %(classname, filename)), )\n",
    "#   print('img shape = ', img_src.shape)\n",
    "#   plt.imshow(img_src)\n",
    "\n",
    "  img = np.zeros((img_src.shape[0]+PATCH_SIZE, img_src.shape[1]+PATCH_SIZE, 3), dtype=int)\n",
    "  img[HALF_PATCH:HALF_PATCH+img_src.shape[0],HALF_PATCH:img_src.shape[1]+HALF_PATCH] = img_src\n",
    "#   plt.imshow(img)\n",
    "\n",
    "  fix = get_fixations(filename, classname, (img_src.shape[0], img_src.shape[1]))\n",
    "#   print(fix)\n",
    "  if len(fix) < MIN_GAZE_POINTS:\n",
    "    return None\n",
    "\n",
    "  patches = np.zeros((len(fix), PATCH_SIZE, PATCH_SIZE, 3), dtype=int)\n",
    "\n",
    "  for i, f in enumerate(fix):\n",
    "    fx = math.floor(f[0]) + HALF_PATCH # to account for paddings\n",
    "    fy = math.floor(f[1]) + HALF_PATCH\n",
    "    p = img[fx-HALF_PATCH:fx+HALF_PATCH,fy-HALF_PATCH:fy+HALF_PATCH]\n",
    "    assert p.shape==(PATCH_SIZE, PATCH_SIZE, 3), 'file \"%s\" of class %s has a patch of shape %s' %(filename, classname, p.shape)\n",
    "    patches[i] = p\n",
    "  return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for extracting features of a patch\n",
    "vgg16 = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(32,32,3), pooling=None, classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = []\n",
    "labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classname in classes:\n",
    "  c_instances = io.loadmat(os.path.join(POET_DIR,'etData','etData_%s.mat' %classname), squeeze_me=True)['etData']\n",
    "  for c_instance in c_instances:\n",
    "    # id\n",
    "    filename = \"%s_%s\" %(classname, c_instance['filename'])\n",
    "    label = class2idx[classname]\n",
    "    partition.append(filename)\n",
    "    if filename in labels.keys():\n",
    "      print(filename)\n",
    "    labels[filename] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del c_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(partition)==DATASET_SIZE\n",
    "assert len(list(labels.keys()))==DATASET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(partition)\n",
    "valid_size = VALIDATION_PERC*len(partition)\n",
    "validation = partition[:int(valid_size)]\n",
    "train = partition[int(valid_size):]\n",
    "assert len(validation)+ len(train)==DATASET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {\n",
    "  'train':train,\n",
    "  'validation':validation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LABELS_FILE, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('idx2class.npy', idx2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('class2idx.npy', class2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORE ALL FEATURES OF ALL PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key - 'class_filename', value - array of patches (MAX_GAZE_POINTS, VGG_NR_FEATURES)\n",
    "storage = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'validation']:\n",
    "  ids = partition[split]\n",
    "  dropped = 0\n",
    "  for i, filename in enumerate(ids):\n",
    "    if filename not in storage.keys():\n",
    "      classname = idx2class[labels[filename]]\n",
    "      patches = gen_patches(filename, classname)\n",
    "      if patches is not None:\n",
    "        p_features = vgg16.predict(patches)\n",
    "        storage[filename] = p_features[:MAX_GAZE_POINTS,0,0:]\n",
    "      else:\n",
    "        dropped += 1\n",
    "print('dropped %s' %dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(PATCHES_FILE, storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
