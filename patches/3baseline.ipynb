{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "from scipy import io\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "POET_DIR = \"D:/data/POETdataset/\"\n",
    "PATCHES_FILE = os.path.join(POET_DIR, \"x.npy\")\n",
    "LABELS_FILE = os.path.join(POET_DIR, 'y.npy')\n",
    "pascal_images = os.path.join(POET_DIR, 'PascalImages')\n",
    "PATCH_SIZE = 32\n",
    "HALF_PATCH = PATCH_SIZE//2\n",
    "DATASET_SIZE = 6270\n",
    "VALIDATION_PERC = 0.2\n",
    "MAX_GAZE_POINTS = 3 # TEMPORARY\n",
    "MIN_GAZE_POINTS = 3\n",
    "VGG_NR_FEATURES = 512\n",
    "user_index = 0 # TEMPORARY\n",
    "BATCH_SIZE = 50\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(POET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [a.split(\"_\")[1].split(\".mat\")[0] for a in glob(\"etData/*\")]\n",
    "idx2class = {i:c for i, c in enumerate(classes)}\n",
    "class2idx = {c:i for i, c in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling=None, classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = vgg16.predict(np.array([img]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We store the feature vectors for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aeroplane',\n",
       " 'bicycle',\n",
       " 'boat',\n",
       " 'cat',\n",
       " 'cow',\n",
       " 'diningtable',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'motorbike',\n",
       " 'sofa']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset():\n",
    "x = []\n",
    "y = []\n",
    "filenames = glob(\"PascalImages/*\")\n",
    "for i, fn in enumerate(filenames):\n",
    "  for cname in classes:\n",
    "    if cname in fn:\n",
    "      y_fn = class2idx[cname]\n",
    "      y.append(y_fn)\n",
    "      img = keras.preprocessing.image.load_img(fn, target_size=(IMG_SIZE, IMG_SIZE, 3))\n",
    "      img = keras.preprocessing.image.img_to_array(img)\n",
    "      feature_vec = vgg16.predict(np.array([img]))\n",
    "      x.append(feature_vec)\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "      #   return np.array(x),np.array(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vgg16-images.npy', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('vgg16-images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 666.,  536.,  504., 1051.,  301.,  498., 1257.,  480.,  510.,\n",
       "         467.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.repeat(-1, len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img = np.zeros((len(x), IMG_SIZE, IMG_SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "y = y.reshape(-1,1)\n",
    "enc.fit(y)\n",
    "y_onehot = enc.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y.npy', y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6270, 512)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(128, activation='relu', input_shape=(512,)))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "optm = keras.optimizers.rmsprop(lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optm,\n",
    "                metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5016 samples, validate on 1254 samples\n",
      "Epoch 1/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 6.7021 - acc: 0.5562 - val_loss: 16.0852 - val_acc: 0.0016\n",
      "Epoch 2/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 6.1472 - acc: 0.5907 - val_loss: 16.1050 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 5.9081 - acc: 0.6091 - val_loss: 16.0923 - val_acc: 7.9745e-04\n",
      "Epoch 4/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 5.6661 - acc: 0.6246 - val_loss: 16.1005 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 5.3740 - acc: 0.6392 - val_loss: 16.0601 - val_acc: 0.0016\n",
      "Epoch 6/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 4.8962 - acc: 0.6607 - val_loss: 16.0695 - val_acc: 7.9745e-04\n",
      "Epoch 7/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 4.7009 - acc: 0.6778 - val_loss: 16.0951 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 4.4383 - acc: 0.6952 - val_loss: 16.1169 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 4.2957 - acc: 0.7055 - val_loss: 16.1106 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 4.1996 - acc: 0.7101 - val_loss: 16.1056 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 4.0378 - acc: 0.7215 - val_loss: 16.1097 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 3.8991 - acc: 0.7285 - val_loss: 16.0950 - val_acc: 7.9745e-04\n",
      "Epoch 13/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 3.9726 - acc: 0.7207 - val_loss: 16.0933 - val_acc: 7.9745e-04\n",
      "Epoch 14/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 3.7337 - acc: 0.7410 - val_loss: 16.0919 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "5016/5016 [==============================] - 0s 100us/step - loss: 3.6574 - acc: 0.7420 - val_loss: 16.0985 - val_acc: 7.9745e-04\n",
      "Epoch 16/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 3.5854 - acc: 0.7522 - val_loss: 16.0969 - val_acc: 7.9745e-04\n",
      "Epoch 17/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 3.5574 - acc: 0.7518 - val_loss: 16.0942 - val_acc: 7.9745e-04\n",
      "Epoch 18/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 3.5668 - acc: 0.7496 - val_loss: 16.0988 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 3.3858 - acc: 0.7580 - val_loss: 16.1031 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 3.3744 - acc: 0.7586 - val_loss: 16.0990 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 3.3081 - acc: 0.7620 - val_loss: 16.0976 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 3.2580 - acc: 0.7665 - val_loss: 16.0999 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 3.1998 - acc: 0.7697 - val_loss: 16.1071 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 3.2043 - acc: 0.7681 - val_loss: 16.0963 - val_acc: 7.9745e-04\n",
      "Epoch 25/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 3.1512 - acc: 0.7709 - val_loss: 16.0987 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 3.1090 - acc: 0.7755 - val_loss: 16.0876 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 3.0565 - acc: 0.7767 - val_loss: 16.0883 - val_acc: 7.9745e-04\n",
      "Epoch 28/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 3.0453 - acc: 0.7787 - val_loss: 16.0859 - val_acc: 7.9745e-04\n",
      "Epoch 29/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.8907 - acc: 0.7891 - val_loss: 16.0873 - val_acc: 7.9745e-04\n",
      "Epoch 30/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.9076 - acc: 0.7885 - val_loss: 16.0432 - val_acc: 0.0016\n",
      "Epoch 31/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.8839 - acc: 0.7931 - val_loss: 16.0691 - val_acc: 7.9745e-04\n",
      "Epoch 32/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.8067 - acc: 0.7976 - val_loss: 16.0880 - val_acc: 7.9745e-04\n",
      "Epoch 33/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.8704 - acc: 0.7897 - val_loss: 16.0498 - val_acc: 7.9745e-04\n",
      "Epoch 34/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.7446 - acc: 0.8012 - val_loss: 16.0677 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.8128 - acc: 0.7883 - val_loss: 16.0865 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.7184 - acc: 0.7978 - val_loss: 16.0724 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.6831 - acc: 0.7982 - val_loss: 16.0432 - val_acc: 0.0016\n",
      "Epoch 38/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.6648 - acc: 0.7984 - val_loss: 16.0689 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.6076 - acc: 0.8080 - val_loss: 16.0501 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.6082 - acc: 0.8110 - val_loss: 16.0389 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.5828 - acc: 0.8054 - val_loss: 16.0649 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "5016/5016 [==============================] - 0s 100us/step - loss: 2.5688 - acc: 0.8108 - val_loss: 16.0494 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.5213 - acc: 0.8110 - val_loss: 16.0672 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.4922 - acc: 0.8132 - val_loss: 15.9927 - val_acc: 7.9745e-04\n",
      "Epoch 45/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.4918 - acc: 0.8116 - val_loss: 15.9899 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.4668 - acc: 0.8124 - val_loss: 16.0417 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.4434 - acc: 0.8154 - val_loss: 16.0409 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.3905 - acc: 0.8202 - val_loss: 16.0258 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.4274 - acc: 0.8168 - val_loss: 15.9485 - val_acc: 7.9745e-04\n",
      "Epoch 50/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.3919 - acc: 0.8168 - val_loss: 16.0003 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "5016/5016 [==============================] - 0s 100us/step - loss: 2.3839 - acc: 0.8186 - val_loss: 16.0037 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.3318 - acc: 0.8244 - val_loss: 16.0335 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.3266 - acc: 0.8244 - val_loss: 16.0367 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.3091 - acc: 0.8262 - val_loss: 15.9807 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.2533 - acc: 0.8260 - val_loss: 16.0237 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.2610 - acc: 0.8287 - val_loss: 16.0091 - val_acc: 7.9745e-04\n",
      "Epoch 57/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.2516 - acc: 0.8264 - val_loss: 16.0305 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.2078 - acc: 0.8371 - val_loss: 16.0056 - val_acc: 7.9745e-04\n",
      "Epoch 59/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.2122 - acc: 0.8317 - val_loss: 15.9828 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.2260 - acc: 0.8293 - val_loss: 15.9307 - val_acc: 7.9745e-04\n",
      "Epoch 61/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.1585 - acc: 0.8327 - val_loss: 15.9389 - val_acc: 7.9745e-04\n",
      "Epoch 62/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.1305 - acc: 0.8359 - val_loss: 15.9558 - val_acc: 7.9745e-04\n",
      "Epoch 63/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 2.1196 - acc: 0.8301 - val_loss: 15.8667 - val_acc: 7.9745e-04\n",
      "Epoch 64/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 2.0579 - acc: 0.8276 - val_loss: 15.6097 - val_acc: 0.0072\n",
      "Epoch 65/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 1.9443 - acc: 0.8311 - val_loss: 15.3805 - val_acc: 0.0088\n",
      "Epoch 66/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 1.8489 - acc: 0.8280 - val_loss: 14.8409 - val_acc: 0.0271\n",
      "Epoch 67/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 1.6673 - acc: 0.8299 - val_loss: 14.4658 - val_acc: 0.0494\n",
      "Epoch 68/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 1.5338 - acc: 0.8309 - val_loss: 14.3092 - val_acc: 0.0455\n",
      "Epoch 69/100\n",
      "5016/5016 [==============================] - 0s 100us/step - loss: 1.4130 - acc: 0.8337 - val_loss: 13.8972 - val_acc: 0.0662\n",
      "Epoch 70/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 1.2936 - acc: 0.8451 - val_loss: 13.9599 - val_acc: 0.0678\n",
      "Epoch 71/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 1.2903 - acc: 0.8381 - val_loss: 13.7583 - val_acc: 0.0670\n",
      "Epoch 72/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 1.1713 - acc: 0.8427 - val_loss: 13.5674 - val_acc: 0.0797\n",
      "Epoch 73/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 1.1387 - acc: 0.8493 - val_loss: 13.7346 - val_acc: 0.0654\n",
      "Epoch 74/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 1.0472 - acc: 0.8493 - val_loss: 13.5776 - val_acc: 0.0726\n",
      "Epoch 75/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 1.0116 - acc: 0.8620 - val_loss: 13.5676 - val_acc: 0.0686\n",
      "Epoch 76/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.9566 - acc: 0.8600 - val_loss: 13.3805 - val_acc: 0.0805\n",
      "Epoch 77/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.9019 - acc: 0.8664 - val_loss: 13.5646 - val_acc: 0.0662\n",
      "Epoch 78/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.8405 - acc: 0.8696 - val_loss: 13.5713 - val_acc: 0.0726\n",
      "Epoch 79/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.8383 - acc: 0.8678 - val_loss: 13.3826 - val_acc: 0.0734\n",
      "Epoch 80/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.8005 - acc: 0.8784 - val_loss: 13.4875 - val_acc: 0.0726\n",
      "Epoch 81/100\n",
      "5016/5016 [==============================] - 0s 100us/step - loss: 0.7987 - acc: 0.8758 - val_loss: 13.6341 - val_acc: 0.0726\n",
      "Epoch 82/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.7506 - acc: 0.8810 - val_loss: 13.5144 - val_acc: 0.0694\n",
      "Epoch 83/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.7268 - acc: 0.8896 - val_loss: 13.4819 - val_acc: 0.0726\n",
      "Epoch 84/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.7121 - acc: 0.8947 - val_loss: 13.4276 - val_acc: 0.0837\n",
      "Epoch 85/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.6892 - acc: 0.8969 - val_loss: 13.5208 - val_acc: 0.0789\n",
      "Epoch 86/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.6780 - acc: 0.8939 - val_loss: 13.4816 - val_acc: 0.0813\n",
      "Epoch 87/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.6445 - acc: 0.9001 - val_loss: 13.6461 - val_acc: 0.0718\n",
      "Epoch 88/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.6170 - acc: 0.9037 - val_loss: 13.5487 - val_acc: 0.0718\n",
      "Epoch 89/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.6343 - acc: 0.9007 - val_loss: 13.6973 - val_acc: 0.0710\n",
      "Epoch 90/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.6052 - acc: 0.9085 - val_loss: 13.6782 - val_acc: 0.0758\n",
      "Epoch 91/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.6049 - acc: 0.9089 - val_loss: 13.4794 - val_acc: 0.0893\n",
      "Epoch 92/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.5546 - acc: 0.9149 - val_loss: 13.5330 - val_acc: 0.0797\n",
      "Epoch 93/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.5458 - acc: 0.9177 - val_loss: 13.5505 - val_acc: 0.0845\n",
      "Epoch 94/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.5572 - acc: 0.9143 - val_loss: 13.5590 - val_acc: 0.0750\n",
      "Epoch 95/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.5483 - acc: 0.9165 - val_loss: 13.5438 - val_acc: 0.0726\n",
      "Epoch 96/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.5509 - acc: 0.9139 - val_loss: 13.3261 - val_acc: 0.0933\n",
      "Epoch 97/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.5332 - acc: 0.9181 - val_loss: 13.6104 - val_acc: 0.0766\n",
      "Epoch 98/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.4883 - acc: 0.9246 - val_loss: 13.5850 - val_acc: 0.0789\n",
      "Epoch 99/100\n",
      "5016/5016 [==============================] - 1s 103us/step - loss: 0.5126 - acc: 0.9215 - val_loss: 13.5225 - val_acc: 0.0805\n",
      "Epoch 100/100\n",
      "5016/5016 [==============================] - 1s 100us/step - loss: 0.4835 - acc: 0.9294 - val_loss: 13.5826 - val_acc: 0.0877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15530ee4c18>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "         x,\n",
    "         y_onehot,\n",
    "         validation_split=0.2,\n",
    "         epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
